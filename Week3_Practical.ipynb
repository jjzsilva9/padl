{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjzsilva9/padl/blob/main/Week3_Practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKXXs11YcqHM"
      },
      "source": [
        "#Regularisation: Ridge and Lasso Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSzLH8OXewlE"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "Imagine a data set which includes two input variables, X<sub>i</sub> and X<sub>j</sub>, X<sub>j</sub>=-X<sub>i</sub>, such that they have no genuine bearing on the output y. Still, a linear model that sets β<sub>j</sub>=-β<sub>i</sub> will have the same accuracy as the one from which X<sub>i</sub> and X<sub>j</sub> are removed, as  the two terms in the sum β<sub>i</sub>X<sub>i</sub>+β<sub>j</sub>X<sub>j</sub> would cancel out. This situation can be mitigated if the loss function of our linear regression, the ordinary least squares (OLS), is extended with an additional *penalty* term, which pushes down the parameters  β<sub>1</sub>... β<sub>p</sub>. Everything else being equal, this would allow us to reduce the parameters β<sub>i</sub> and β<sub>j</sub> of our example to as close to zero as possible without any loss of accuracy.\n",
        "\n",
        "When the penalty term is proportional to L2 = Σ(β<sub>i</sub><sup>2</sup>), the resulting regression is known as [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn-linear-model-ridge) regression (with a loss function OLS+λ.L2), while using penalty term L1 = Σ|β<sub>i</sub>| (loss function OLS+λ.L1) corresponds to the so-called [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)  regression.\n",
        "\n",
        "L1 and L2 are also known as *regularisation* terms. λ is referred to as the complexity parameter. You can also see the letter α (alpha) used instead of λ - this includes the entire scikit-learn documentation.\n",
        "\n",
        "Using regularisation terms - L1, L2 or their combination (with the result known as [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elastic%20net#sklearn-linear-model-elasticnet)) has as an effect a reduction of the variance of the model parameters β<sub>1</sub>... β<sub>p</sub> as we vary the training data. The price to pay is a larger model bias, i.e. the average (squared) error we end up with as we vary our training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ozqzDKYbmj"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "In this exercise you will use both Ridge and Lasso regression, and also plain Linear Regression to build regression models for the California house prices dataset. The task for this dataset is to learn a model that predicts the median price of a house (in a California district) from 8 variables describing that district.\n",
        "\n",
        "To choose the correct complexity penalty for Ridge and Lasso regression you should use the built-in *cross-validation* (see online lecture) that is available in scikit-learn via the classes [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html?highlight=ridgecv#sklearn.linear_model.RidgeCV) and [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Look up how to use them in the scikit-learn documentation.\n",
        "\n",
        "To help you out here is the initial part of my Python program for doing this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrCqA9LtW2Zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e70f954-ee38-45c9-c70b-12f66c237733"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import RidgeCV, LinearRegression, LassoCV\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "california = fetch_california_housing()\n",
        "print(california.data.shape)\n",
        "print(california.target.shape)\n",
        "print(california.feature_names)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n",
            "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HT2LBLgZ2zu"
      },
      "source": [
        "**In your first experiment:**\n",
        "\n",
        "*   learn from the first 15,000 datapoints\n",
        "*   print out the learned parameter values for each predictor\n",
        "*   compute the R<sup>2</sup> score on the remainder.\n",
        "\n",
        "Do this for linear regression, ridge regression and lasso regression, then:\n",
        "\n",
        "*   For ridge and lasso regression print out the complexity penalty value cross-validation found.\n",
        "*   Inspect the learned parameter values for each predictor, and comment on their significance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl6o4ZFyb_Jo"
      },
      "source": [
        "**In your second experiment:**\n",
        "\n",
        "*  do the same except this time just train on the first 150 datapoints.\n",
        "\n",
        "The results you get from the first and second experiments should be quite different - consider the reasons for that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcWjlJVYrjZJ"
      },
      "source": [
        "##Solution\n",
        "\n",
        "(to the above two experiments combined)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfWKPDUAZ2U1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2cb336-2d51-47b9-bff3-b9c5bd6f9136"
      },
      "source": [
        "#Add your code here\n",
        "linear_regressor = LinearRegression().fit(california.data[:15000], california.target[:15000])\n",
        "y_predict = linear_regressor.predict(california.data[15000:])\n",
        "print(\"Linear Regression\")\n",
        "print(f\"Learned parameter values: {linear_regressor.coef_}\")\n",
        "print(f\"R^2 Score: {r2_score(california.target[15000:], y_predict)}\")\n",
        "\n",
        "ridge_regressor = RidgeCV().fit(california.data[:15000], california.target[:15000])\n",
        "y_predict_ridge = ridge_regressor.predict(california.data[15000:])\n",
        "print(\"\\nRidge Regression\")\n",
        "print(f\"Learned parameter values: {ridge_regressor.coef_}\")\n",
        "print(f\"R^2 Score: {r2_score(california.target[15000:], y_predict_ridge)}\")\n",
        "print(ridge_regressor.alpha_)\n",
        "\n",
        "lasso_regressor = LassoCV().fit(california.data[:15000], california.target[:15000])\n",
        "y_predict_lasso = lasso_regressor.predict(california.data[15000:])\n",
        "print(\"\\nLasso Regression\")\n",
        "print(f\"Learned parameter values: {lasso_regressor.coef_}\")\n",
        "print(f\"R^2 Score: {r2_score(california.target[15000:], y_predict_lasso)}\")\n",
        "print(lasso_regressor.alpha_)\n",
        "\n",
        "print(\"\\n150 values:\")\n",
        "linear_regressor = LinearRegression().fit(california.data[:150], california.target[:150])\n",
        "y_predict = linear_regressor.predict(california.data[15000:])\n",
        "print(\"Linear Regression\")\n",
        "print(f\"Learned parameter values: {linear_regressor.coef_}\")\n",
        "print(f\"R^2 Score: {r2_score(california.target[15000:], y_predict)}\")\n",
        "\n",
        "ridge_regressor = RidgeCV().fit(california.data[:150], california.target[:150])\n",
        "y_predict_ridge = ridge_regressor.predict(california.data[15000:])\n",
        "print(\"\\nRidge Regression\")\n",
        "print(f\"Learned parameter values: {ridge_regressor.coef_}\")\n",
        "print(f\"R^2 Score: {r2_score(california.target[15000:], y_predict_ridge)}\")\n",
        "print(ridge_regressor.alpha_)\n",
        "\n",
        "lasso_regressor = LassoCV().fit(california.data[:150], california.target[:150])\n",
        "y_predict_lasso = lasso_regressor.predict(california.data[15000:])\n",
        "print(\"\\nLasso Regression\")\n",
        "print(f\"Learned parameter values: {lasso_regressor.coef_}\")\n",
        "print(f\"R^2 Score: {r2_score(california.target[15000:], y_predict_lasso)}\")\n",
        "print(lasso_regressor.alpha_)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression\n",
            "Learned parameter values: [ 4.43133772e-01  6.88834433e-03 -1.06225623e-01  6.21297542e-01\n",
            " -1.22453044e-05 -7.81556745e-03 -3.85358132e-01 -3.67814616e-01]\n",
            "R^2 Score: 0.5931322421964103\n",
            "\n",
            "Ridge Regression\n",
            "Learned parameter values: [ 4.41814520e-01  6.90517633e-03 -1.03797745e-01  6.08709327e-01\n",
            " -1.21622315e-05 -7.82167968e-03 -3.85138991e-01 -3.67306455e-01]\n",
            "R^2 Score: 0.5928319083614983\n",
            "10.0\n",
            "\n",
            "Lasso Regression\n",
            "Learned parameter values: [ 3.84587478e-01  8.66872109e-03  4.28264823e-03  0.00000000e+00\n",
            " -6.33382373e-06 -7.66105119e-03 -3.07238661e-01 -2.69380888e-01]\n",
            "R^2 Score: 0.5514968914697892\n",
            "0.02658642580705467\n",
            "\n",
            "150 values:\n",
            "Linear Regression\n",
            "Learned parameter values: [ 2.22526945e-01  2.61472099e-03 -8.48786316e-02 -4.99511973e-01\n",
            "  7.86295457e-05  2.65034648e-02  2.05076257e+00  1.69609083e+01]\n",
            "R^2 Score: -723.3836360061374\n",
            "\n",
            "Ridge Regression\n",
            "Learned parameter values: [ 3.70750674e-01  3.94284839e-03 -1.08731917e-01 -8.81075799e-01\n",
            "  1.43208724e-04 -1.34150826e-02 -3.83228144e-01  3.80682810e+00]\n",
            "R^2 Score: -55.60173831930965\n",
            "0.1\n",
            "\n",
            "Lasso Regression\n",
            "Learned parameter values: [ 3.58882586e-01 -0.00000000e+00 -1.96050078e-02 -0.00000000e+00\n",
            "  2.16566027e-04 -0.00000000e+00 -0.00000000e+00  0.00000000e+00]\n",
            "R^2 Score: 0.3687372043606524\n",
            "0.10094685133600002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7uEQm0DcvqO"
      },
      "source": [
        "##L1 and L2 regularisation with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMdh4fj1iQa1"
      },
      "source": [
        "So, Lasso regression is simply a regression model that uses L1 regularisation;  a model that uses L2 regularisation is called Ridge regression. This means that we can easily modify our implementation of Linear regression by tweaking the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YolUjUfCXufP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287f17c3-3407-4d9e-b635-c4f29460e665"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Function to compute the L1 loss of all weights in a model\n",
        "def L1regloss(model):\n",
        "  reg_loss = 0\n",
        "  for param in model.parameters():\n",
        "    reg_loss += param.abs().sum()\n",
        "  return reg_loss\n",
        "\n",
        "def L2regloss(model):\n",
        "  reg_loss = 0\n",
        "  for param in model.parameters():\n",
        "    reg_loss += (param**2).sum()\n",
        "  return reg_loss\n",
        "\n",
        "model = torch.nn.Linear(8,1)\n",
        "X_train = torch.from_numpy(np.float32(california.data[:15000,:]))\n",
        "y_train = torch.from_numpy(np.float32(california.target[:15000])).unsqueeze(1)\n",
        "X_test = torch.from_numpy(np.float32(california.data[15000:,:]))\n",
        "y_test = torch.from_numpy(np.float32(california.target[15000:])).unsqueeze(1)\n",
        "epochs = 50\n",
        "\n",
        "# Setup optimiser\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-7)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(epochs):\n",
        "    y_predict = model(X_train)\n",
        "    fit_loss = criterion(y_train,y_predict)\n",
        "    # Use L1regloss for Lasso\n",
        "    reg_loss = L2regloss(model)\n",
        "    loss = fit_loss+reg_loss\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    print('epoch {}, fit loss {}, reg loss {}'.format(epoch, fit_loss.item(), reg_loss.item()))\n",
        "\n",
        "# Generalisation error\n",
        "print(criterion(model(X_test),y_test))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, fit loss 3925.813720703125, reg loss 0.06500545889139175\n",
            "epoch 1, fit loss 421.3731994628906, reg loss 0.06400217115879059\n",
            "epoch 2, fit loss 50.99968338012695, reg loss 0.06390152126550674\n",
            "epoch 3, fit loss 11.84511661529541, reg loss 0.06389054656028748\n",
            "epoch 4, fit loss 7.694989204406738, reg loss 0.06388718634843826\n",
            "epoch 5, fit loss 7.24427604675293, reg loss 0.06388403475284576\n",
            "epoch 6, fit loss 7.1845622062683105, reg loss 0.06388070434331894\n",
            "epoch 7, fit loss 7.166201591491699, reg loss 0.06387729942798615\n",
            "epoch 8, fit loss 7.1522417068481445, reg loss 0.06387388706207275\n",
            "epoch 9, fit loss 7.138779640197754, reg loss 0.06387045234441757\n",
            "epoch 10, fit loss 7.125399112701416, reg loss 0.06386702507734299\n",
            "epoch 11, fit loss 7.112058162689209, reg loss 0.063863605260849\n",
            "epoch 12, fit loss 7.098752021789551, reg loss 0.06386019289493561\n",
            "epoch 13, fit loss 7.085480690002441, reg loss 0.06385678797960281\n",
            "epoch 14, fit loss 7.072244167327881, reg loss 0.06385339051485062\n",
            "epoch 15, fit loss 7.059040069580078, reg loss 0.06385000795125961\n",
            "epoch 16, fit loss 7.045871257781982, reg loss 0.06384661048650742\n",
            "epoch 17, fit loss 7.0327348709106445, reg loss 0.0638432428240776\n",
            "epoch 18, fit loss 7.0196332931518555, reg loss 0.0638398751616478\n",
            "epoch 19, fit loss 7.006564140319824, reg loss 0.06383650749921799\n",
            "epoch 20, fit loss 6.993530750274658, reg loss 0.06383315473794937\n",
            "epoch 21, fit loss 6.980528354644775, reg loss 0.06382980197668076\n",
            "epoch 22, fit loss 6.9675612449646, reg loss 0.06382646411657333\n",
            "epoch 23, fit loss 6.954626560211182, reg loss 0.06382311880588531\n",
            "epoch 24, fit loss 6.9417243003845215, reg loss 0.06381978839635849\n",
            "epoch 25, fit loss 6.928856372833252, reg loss 0.06381646543741226\n",
            "epoch 26, fit loss 6.91602087020874, reg loss 0.06381314247846603\n",
            "epoch 27, fit loss 6.903219699859619, reg loss 0.0638098269701004\n",
            "epoch 28, fit loss 6.8904500007629395, reg loss 0.06380652636289597\n",
            "epoch 29, fit loss 6.877713203430176, reg loss 0.06380323320627213\n",
            "epoch 30, fit loss 6.865009307861328, reg loss 0.06379994750022888\n",
            "epoch 31, fit loss 6.852337837219238, reg loss 0.06379666179418564\n",
            "epoch 32, fit loss 6.839698314666748, reg loss 0.06379338353872299\n",
            "epoch 33, fit loss 6.82709264755249, reg loss 0.06379010528326035\n",
            "epoch 34, fit loss 6.814518928527832, reg loss 0.06378684192895889\n",
            "epoch 35, fit loss 6.801977634429932, reg loss 0.06378358602523804\n",
            "epoch 36, fit loss 6.789468765258789, reg loss 0.06378033012151718\n",
            "epoch 37, fit loss 6.776992321014404, reg loss 0.06377707421779633\n",
            "epoch 38, fit loss 6.764547348022461, reg loss 0.06377384066581726\n",
            "epoch 39, fit loss 6.752134323120117, reg loss 0.06377061456441879\n",
            "epoch 40, fit loss 6.73975133895874, reg loss 0.06376738846302032\n",
            "epoch 41, fit loss 6.727403163909912, reg loss 0.06376415491104126\n",
            "epoch 42, fit loss 6.715085983276367, reg loss 0.06376094371080399\n",
            "epoch 43, fit loss 6.7027997970581055, reg loss 0.06375773996114731\n",
            "epoch 44, fit loss 6.690545082092285, reg loss 0.06375453621149063\n",
            "epoch 45, fit loss 6.6783223152160645, reg loss 0.06375134736299515\n",
            "epoch 46, fit loss 6.666130065917969, reg loss 0.06374815851449966\n",
            "epoch 47, fit loss 6.653969764709473, reg loss 0.06374496966600418\n",
            "epoch 48, fit loss 6.641839504241943, reg loss 0.0637417882680893\n",
            "epoch 49, fit loss 6.629741668701172, reg loss 0.0637386217713356\n",
            "tensor(6.4291, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNLtKvAni9ZJ"
      },
      "source": [
        "**To do:**\n",
        "\n",
        "Modify the loss function in the above solution to use the L2 loss as penalty instead of L1 loss, thus obtaining an implementation of Ridge regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG7ylmpmjltV"
      },
      "source": [
        "# Your code here - or edit the code above."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}